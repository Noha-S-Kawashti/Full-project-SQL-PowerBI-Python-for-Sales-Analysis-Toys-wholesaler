# -*- coding: utf-8 -*-
"""Final Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YyeGU5FrkHaSR6k8o0AoV5qNsqYwyh21
"""

#### Import need Libraries ####
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#### Reading the data ####
df= pd.read_csv('/content/drive/MyDrive/Final Project - DA Carerha /sales_data_sample.csv',encoding='latin-1')
df.head()

# Noticed that not all the Columns are appearing, for example CUSTOMERNAME is not shown, so I expanded the range of Display :
pd.set_option('display.max_columns', 30)
df.head()

df.info()
#________________________________________________________

#### Data Cleaning & Transformation ####

## Removing unnecessary Columns :
df=df.drop(['PRICEEACH','MSRP','PHONE','POSTALCODE','ADDRESSLINE1','ADDRESSLINE2','CONTACTLASTNAME','CONTACTFIRSTNAME'],axis=1)
df.head()
#____________________________________________________________
## Change Data Type ##

# Change "PRODUCTLINE" data type from object to String :
df['PRODUCTLINE']=df['PRODUCTLINE'].astype('string')

# Change ORDERDATE data Type :

df['ORDERDATE']=df['ORDERDATE'].str.replace(' 0:00','')
df.head()
df.info()
#________________________________________________________

df['ORDERDATE']=pd.to_datetime(df['ORDERDATE'],format="%m/%d/%Y")
df.head()
df.info()

# Changed the data type of ORDERDATE successfully.
#________________________________________________________

# Checking overview of Null value :
df.isnull().sum()

# Analyzing the Null outputs in "TERRITORY" :
df[df['TERRITORY'].isnull()]

# Changing in “Territory” the “NA” records to “North America”, so as to be read as “non-null” :
df['TERRITORY']=df['TERRITORY'].fillna('North America')
df.head(15)

df.isnull().sum()
## Now the only "Null" values are in the column “STATE” which is not an issue (clarified in Data Exploration section in Report)

# Remove the column of "STATE" as it's not important, and logically has some Null values for Countries that normally have only Cities not states :
df=df.drop('STATE',axis=1)
df.head()
#________________________________________________________

# In TERRITORY; “Japan” should be replaced to be “APAC”, (clarified in Data Exploration section in Report) :

df['TERRITORY']=df['TERRITORY'].str.replace('Japan','APAC')
df.loc[df['COUNTRY'].str.contains('Japan')]
df.head(40)
#________________________________________________________

#### Data Analysis ####

# To get a quick Statistical overview of the Numerical columns' data in the dataset :

Num_col=['QUANTITYORDERED','SALES']
df[Num_col].describe()
#________________________________________________________

# from the Statistical overview, I want to know which customer made the highest Sales transaction orderline (the MAX = 14082.8) :

df.loc[df['SALES']==14082.8]

## so it’s found that the Customer “The Sharp Gifts Warehouse” has the highest sales Transaction line as a part of ORDERNUMBER 10407 on 22-Apr-2005, the whole order of 12 lines has all its items "Cars". Customer is located in USA, in state of California.
## Also the average (Mean) Sales is 3553.88 among all sales data.
#________________________________________________________

# Get an overview on TERRITORYs to detect the most frequent one, that's most repeated among OrderLines transactions (as a percentage) :

percentages=df['TERRITORY'].value_counts(normalize=True)*100
percentages.round(2)

## It's found that EMEA territory (Europe, the Middle East, Africa) makes the most frequent OrderLine transactions among all ORDERLINENUMBERs (alomst half (50%)).
#________________________________________________________

#### Combine similar product descriptions for consistency :

# Classic Cars & Vintage Cars can better be combined together in one new Category ProductLine "Cars" :

df['PRODUCTLINE']=df['PRODUCTLINE'].str.replace('Classic Cars','Cars')
df['PRODUCTLINE']=df['PRODUCTLINE'].str.replace('Vintage Cars','Cars')
df['PRODUCTLINE'].value_counts()
## Now it's successfully changed, so it has become now 6 Prodcutlines in total.
#________________________________________________________

# Total spending amount per Customer :
customer_spending=df.groupby('CUSTOMERNAME')['SALES'].sum()

print(customer_spending.max())
print(customer_spending.min())
## It's found that 912,294.11 is the highest Spending by a customer, and 9,129.35 is the Least spending by a customer. I will use this info. to decide Segmentation limit value of Customer spending...

# For segmentation ; Assume that High-value customers starts from spending of 100000, and Low-value is below 100000 :

print((customer_spending[customer_spending>100000]).count())
print((customer_spending[customer_spending<100000]).count())
## So the total customers of 92: Can be classified to 38 Customers of High-value, and 54 customers of Low-value.

#### Adding a new Column of "Customer Value" of High-value and Low-value, to segment customers per their spending :
df['Customer Value']=df['CUSTOMERNAME'].apply(lambda x: 'High-value' if customer_spending[x]>100000 else 'Low-value')
df.head()
#________________________________________________________

df['CUSTOMERNAME'].nunique()

# Another aspect that can be valuable while looking into the spending size is to consider the "DEALSIZE" column,
# but I see that the above segmentation by grouping SALES is more accurate for the original Business Question.

# CustomerNames with most No. of times of Large Deal Sizes / High-value :
Customer_deal_size=df[df['DEALSIZE']=='Large'].groupby('CUSTOMERNAME')['DEALSIZE'].count()
print(Customer_deal_size[Customer_deal_size>5])
## It's found that 3 Customers had the highest no. of "Large" Deal Sizes, customers are : "Euro Shopping Channel", "Mini Gifts Distributors Ltd.", "The Sharp Gifts Warehouse".
#________________________________________________________

# Purchase frequency per each Customer :
Customer_frequency=df.groupby('CUSTOMERNAME')['ORDERNUMBER'].nunique()

print(Customer_frequency.max())
print(Customer_frequency.min())
## It's found that the highest number of Orders per one customer is 26, while the least no. of orders by a customer was 1.
## I will use this info. to decide Segmentation limit of Customer Frequency ...

df.loc[df['CUSTOMERNAME']==Customer_frequency[Customer_frequency==26].index[0]]
## "Euro Shopping Channel" is found to be the customer with the highest no. of Orders (26).

df.loc[df['CUSTOMERNAME']==Customer_frequency[Customer_frequency==1].index[0]]
## "Bavarian Collectables Imports, Co." is the customer with only one Order.

#### Adding a new Column of "Customer PO Freq." indicating number of Purchase orders (PO frequency) per customer :
df['Customer PO Freq.']=df['CUSTOMERNAME'].apply(lambda x: Customer_frequency[x])
df.head()

#### Adding a new Column of "Customer Frequency" of High-freq. and Low-freq., to segment customers per their number of orders (purchase Frequency) :
df['Customer Frequency']=df['CUSTOMERNAME'].apply(lambda x: 'High-freq.' if Customer_frequency[x]>14 else 'Low-freq.')
df.head()
#________________________________________________________

#### Calculate customer lifetime value (CLTV) based on purchase history :

##Customer Lifetime Value CLTV = (Customer Value * Average Customer Lifespan)
#________________________________________________________
# First, Calculate Average Customer Lifespan :
# Average Customer Lifespan = sum of customer Lifespans / Number of customers

customer_max_date=df.groupby('CUSTOMERNAME')['ORDERDATE'].max()
customer_min_date=df.groupby('CUSTOMERNAME')['ORDERDATE'].min()

customer_lifespan=(customer_max_date-customer_min_date)

# Convert the customer_lifespan period from Days to Months :
customer_lifespan_months = customer_lifespan.dt.days//30

# Calculate the Sum of Customer Lifespans :
sum_customer_lifespans=customer_lifespan_months.sum()
print(sum_customer_lifespans)
## It's found that the Sum of Active periods of customers was Approximately 1275 Months in total, based on Past data.

# So will use this "sum of customer Lifespans" to calculate the "Average Customer Lifespan" :
Average_customer_lifespan =sum_customer_lifespans//df['CUSTOMERNAME'].nunique()
print(Average_customer_lifespan)
## It's found that the Average Customer Lifespan is approximately 14 Months, this period is based on Past data,
# and not the total Expected period to have the Customer actively with our company.
#________________________________________________________

# Second, Calculate "Customer Value" :
# Customer Value = Average Purchase Value x Average Number of Purchases

# Use the (Customer_spending) that was calculated in previous requirement :
customer_spending=df.groupby('CUSTOMERNAME')['SALES'].sum()

Avg_purchase_value_per_customer =customer_spending//df['ORDERNUMBER'].nunique()
Avg_purchase_value_per_customer
## So this is the record of "Average Purchase Value" per each customer.
#________________________________________________________

# Calculate "Average Number of Purchases" :
Avg_number_of_purchases =df['ORDERNUMBER'].nunique()//df['CUSTOMERNAME'].nunique()
Avg_number_of_purchases
## The Average Number of Purchases is 3 Orders.
#________________________________________________________
Customer_Value =Avg_purchase_value_per_customer*Avg_number_of_purchases
Customer_Value
## So this is the record of "Customer Value" per each customer.
#________________________________________________________

# Finally, Calculate CLTV "Customer Lifetime Value" :
# CLTV = (Customer Value * Average Customer Lifespan)

CLTV=Customer_Value*Average_customer_lifespan
CLTV

## So CLTV is calculated per each customer,
# Now I shall add a new Column for CLTV in the Table to be clearly visible for each of the 92 customers :

df['CLTV']=df['CUSTOMERNAME'].apply(lambda x: CLTV[x])
df.head()
#________________________________________________________

# Additionally, we can get the CLTV as an aggreagte value for the whole data :

CLTV_aggregately =CLTV.sum()//df['CUSTOMERNAME'].nunique()
# CLTV aggr. = 1272609/92
CLTV_aggregately

## This is the general CLTV (13,832).
#________________________________________________________

"""Analyze trends over time (e.g., monthly, quarterly) to identify seasonal
buying patterns or changes in customer behavior :
"""

# Sales by months :

Sales_per_month=df.groupby('MONTH_ID')['SALES'].sum()
Sales_per_month

Orders_per_month=df.groupby('MONTH_ID')['ORDERNUMBER'].nunique()
Orders_per_month

## It's found that "November" is the month with the highest Buying pattern in terms of Purchase value and also Number of Orders,
## there's is a massive shift in this month than the rest of the year, and it's followed by October in high Buying pattern.
#_____________________________________________________________
# Sales by Quarter :

Sales_per_QTR=df.groupby('QTR_ID')['SALES'].sum()
Sales_per_QTR

Orders_per_QTR=df.groupby('QTR_ID')['ORDERNUMBER'].nunique()
Orders_per_QTR

## It's found that the Forth Quarter has the highest Buying pattern by the customers,
## in terms of both total Purchase value and also Number of Orders.

# I will not make Buying analysis per Year, becasue I see it will not be insightful, since that Year 2005 is not complete (has only 5 months).
#________________________________________________________

"""**Draw a Line Chart, for visualizing the Time trends in customer Purchase value :**"""

# Group sales by Month and year:
sales_by_month_year = df.groupby(['YEAR_ID','MONTH_ID'])['SALES'].sum().unstack()

# Plot lines for each year :
for year in sales_by_month_year.index:
    plt.plot(sales_by_month_year.columns,sales_by_month_year.loc[year],label=year,marker='o')

# Customize the plot :
plt.xlabel('Month')
plt.ylabel('Sales')
plt.title('Monthly Sales trends by year')
plt.legend(title='Year')
plt.xticks(sales_by_month_year.columns)  # Set x-ticks to month IDs

plt.show()
#________________________________________________________

## Sales amounby Category :
Category_sales=df.groupby('PRODUCTLINE')['SALES'].sum()
Category_sales

# Calculate percentages
Category_sales_percentage = Category_sales / Category_sales.sum() * 100
print(Category_sales_percentage)

## Let's visualize the Sales amount percentage by Category :

plt.figure(figsize = (7,7))
plt.pie(Category_sales_percentage,
        labels = Category_sales_percentage.index,
        autopct='%1.0f%%'
        )
plt.legend(loc = 'upper right')
plt.title('Sales percentage by Category')
#________________________________________________________

# Purchase Orders by  City in countries :
City_orders=df.groupby(['COUNTRY','CITY'])['ORDERNUMBER'].nunique()
City_orders
#___________________________________________

# Plot this insight in Pivot Table :
check =pd.pivot_table(
    df,
    index = ['CITY','COUNTRY'],
    values = ['ORDERNUMBER'],
    aggfunc = 'nunique'
)

# Reset index to convert it into normal columns :
check = check.reset_index()

# Sort by 'ORDERNUMBER' in descending order :
check =check.sort_values('ORDERNUMBER',ascending=False)
check

## It's found that Madrid City in Spain has the highest Number of Orders (31),
## while Munich in Germany has only 1 purchase Order.
#________________________________________________________

# Visualize CLTV by Customers in Territories :

Customers_CLTV_in_Territory=df.groupby('TERRITORY')['CLTV'].sum()
Customers_CLTV_in_Territory

# Plot CLTV by Territory, to know Territories Values (ratio) :

# Calculate percentages
Territory_percentage_by_CLTV = Customers_CLTV_in_Territory / Customers_CLTV_in_Territory.sum() * 100
print(Territory_percentage_by_CLTV)

## Let's visualize the Sales amount percentage by Category :

plt.figure(figsize = (7,7))
plt.pie(Territory_percentage_by_CLTV,
        labels = Territory_percentage_by_CLTV.index,
        autopct='%1.0f%%'
        )
plt.legend(loc = 'upper right')
plt.title('Territory percentage based on CLTV')

## It's found that EMEA has teh highest CLTV , then followed by North America
#________________________________________________________

# Most sold item (QTY) of Product Lines along Months of years :

# Sort the DataFrame by 'MONTH_ID'
df_sorted = df.sort_values(by=['YEAR_ID', 'MONTH_ID'])

sns.lineplot(
    data=df_sorted.groupby(['YEAR_ID', 'MONTH_ID', 'PRODUCTLINE'])['QUANTITYORDERED'].sum().reset_index(),
    x='MONTH_ID',
    y='QUANTITYORDERED',
    hue='PRODUCTLINE',
    marker='o'
)
plt.xlabel('Month')
plt.ylabel('Quantity Ordered')
plt.title('Most sold item (QTY) of Product Lines along months of years')
plt.legend(title='Product Line')
plt.show()
#________________________________________________________

df.head()
df.info()

df.to_csv('/content/drive/My Drive/df.csv',index=False)